{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CHNHV9ZBkWZ"
      },
      "source": [
        "# Further Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbUt6JtKuzOx",
        "outputId": "c5e51e8e-4811-4d6b-960e-8cff76583d55"
      },
      "outputs": [],
      "source": [
        "# Developers: Charles Cutler and Christopher Samson\n",
        "# Class: CMSC 516 Advanced Natural Language Processing\n",
        "# The following code was developed for the first programming assignment in the course\n",
        "# CMSC 516, Advanced Natural Language Processing, at Virginia Commonwealth University\n",
        "#\n",
        "# This program was created with the intention of combining what we have learned so far in the course to classify the\n",
        "# sentiment of Twitter tweets. Sentiment analysis is important when it comes to extracting important information from\n",
        "# text. Classifying the sentiment of tweets from Twitter specifically is a difficult task due to the informal language\n",
        "# used. Sentiment classification is determining whether a given sentence or group of sentences reflects a positive\n",
        "# (think happy or good) sentiment or a negative (think upset or bad) sentiment. The method by which I went about the\n",
        "# task of sentiment analysis is by using a supervised machine learning and extracting features from a set of manually\n",
        "# classified tweets.\n",
        "#\n",
        "# REMINDER!!! READ ME!!!\n",
        "# Two options to load data into this file for use in training.\n",
        "#\n",
        "# <><><><><><><><><><><><><><><><><><>><><><><><><><><><><><><><><><><><><><><><><><>\n",
        "#\n",
        "# OPTION 1: GOOGLE COLAB ( RECOMMENDED )\n",
        "#\n",
        "# Step 1) Upload the clean version of the data named \"Cleaned_Sentiment140_Data.csv\" \n",
        "#    and the Glove static word embeddings named \"glove.twitter.27B.200d.txt\" to you google drive.\n",
        "#\n",
        "# Step 2) UNCOMMENT the following two lines to allow google colab to mount your google drive: \n",
        "#\n",
        "# from google.colab import drive ## UNCOMMENT ME, THIS IS ONE LINE!!\n",
        "# drive.mount('/content/drive') ## UNCOMMENT ME, THIS IS THE OTHER LINE!!\n",
        "#\n",
        "# Step 3) USE the follwing two lines in place of a local file path name. \n",
        "#   NOTICE: If you upload these files into a subfolder you must change the google drive paths below to match!!!\n",
        "#\n",
        "#   glove_file_name = str('/content/drive/MyDrive/glove.twitter.27B.200d.txt') ## CURRENTLY I AM IN THE CODE BELOW!\n",
        "#   data_file_name = str('/content/drive/MyDrive/Cleaned_Sentiment140_Data.csv') ## CURRENTLY I AM IN THE CODE BELOW\n",
        "#\n",
        "# <><><><><><><><><><><><><><><><><><>><><><><><><><><><><><><><><><><><><><><><><><>\n",
        "#\n",
        "# OPTION 2: LOCALLY\n",
        "# 1) Replace the file pathnames in the following two lines with the local file paths to the \n",
        "# clean version of the data named \"Cleaned_Sentiment140_Data.csv\" and the Glove static word \n",
        "# embeddings named \"glove.twitter.27B.200d.txt\":\n",
        "#\n",
        "#   glove_file_name = str('/content/drive/MyDrive/glove.twitter.27B.200d.txt')   ## I LOOK LIKE THIS, REPLACE ME BELOW!\n",
        "#   data_file_name = str('/content/drive/MyDrive/Cleaned_Sentiment140_Data.csv') ## I LOOK LIKE THIS, REPLACE ME BELOW!\n",
        "#\n",
        "# <><><><><><><><><><><><><><><><><><>><><><><><><><><><><><><><><><><><><><><><><><>\n",
        "\n",
        "# If you do not use Google Colab, make sure to install these python libraries.\n",
        "# Installation instructions can be found at:\n",
        "#\n",
        "# https://www.tensorflow.org/install\n",
        "# https://numpy.org/install/\n",
        "# https://www.tutorialspoint.com/keras/keras_installation.htm\n",
        "# https://scikit-learn.org/stable/install.html\n",
        "#\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import csv\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras import optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For debugging purposes or incase you need to verify the version of Tensor Flow\n",
        "print(tf.__version__)\n",
        "\n",
        "# These are file pathnames, IN GOOGLE DRIVE WHEN USING GOOGLE COLAB, to the clean version of the data named \n",
        "# \"Cleaned_Sentiment140_Data.csv\" and the Glove static word embeddings named \"glove.twitter.27B.200d.txt\".\n",
        "#\n",
        "# Alternatively, these CAN be replaced with local file pathnames to the same files if they are downloaded \n",
        "# to your computer and you intend to run this code locally.\n",
        "glove_file_name = str('/content/drive/MyDrive/glove.twitter.27B.200d.txt')\n",
        "data_file_name = str('/content/drive/MyDrive/Cleaned_Sentiment140_Data.csv')\n",
        "\n",
        "# Adjust these hyperparmeters in the case of memory issues or in the case of curiosity\n",
        "lr = 0.0001 # Learning Rate\n",
        "bs = 320 # Batch Size\n",
        "epochs = 10 # Number of Epochs\n",
        "\n",
        "# These are dependent on the static word embeddings you use to represent the words of a tweet, for example.\n",
        "#\n",
        "# The \"numberOfEmbeddings\" is the number of static word embeddings that appear in you desired word embeddings datafile\n",
        "#\n",
        "# The \"dimensions\" is the number of \"features\" or numbers in each static word embedding. \n",
        "# We used \"glove.twitter.27B.200d.txt\" which, for each static word embedding, provides numerical vectors of length 200 \n",
        "# for each word in its datafile.\n",
        "numberOfEmbeddings = 1193515\n",
        "dimensions = 200\n",
        "\n",
        "# Used to build the collection of static word embeddings \n",
        "embeddings = np.zeros((numberOfEmbeddings, dimensions))\n",
        "\n",
        "# Build a hashmap to later convert tweets from lists of words into lists of integers. \n",
        "# These integers represent the numerical position in \"embeddings_indexes\" where the word that was previously\n",
        "# in the tweet at that position is located. \n",
        "# \n",
        "# Together \"embeddings\" and \"embeddings_indexes\" allow for the neural network to create a numerical matrix for each tweet. \n",
        "# The main idea is that this numerical matrix not only represents the words in a tweet but also captures something more that \n",
        "# the neural network can learn. \n",
        "embeddings_indexes = {}\n",
        "embeddings_indexes[\"padding_token\"] = 0\n",
        "\n",
        "\n",
        "tweetVectors = [] \n",
        "classifications = []\n",
        "\n",
        "# Go through all of the static word embeddings in the Glove datafile and \n",
        "# extract the word and corresponding numerical static word embedding \n",
        "with open(glove_file_name) as glove_200embeddings:\n",
        "\n",
        "    lines = glove_200embeddings.readlines()\n",
        "    # At this point we have not seen any words nor their word embeddings\n",
        "    index = 0\n",
        "    for line in lines:\n",
        "\n",
        "        tokenized_data_line = line.split()\n",
        "\n",
        "        # In the case that a static word embedding does not get read by the computer correctly we skip over it\n",
        "        if len(tokenized_data_line) < dimensions+1:\n",
        "            continue\n",
        "\n",
        "        # Extract the word\n",
        "        embedding_text = tokenized_data_line[0] \n",
        "        \n",
        "        # Extract the numerical static word embedding and convert it into a float\n",
        "        embedding_values = np.asarray(tokenized_data_line[1:], dtype=np.float32) \n",
        "        \n",
        "        # Remember where this word is to be placed in the embedding collection\n",
        "        embeddings_indexes[embedding_text] = index + 1\n",
        "\n",
        "        # Place the numerical static word embedding for a word in the embedding collection\n",
        "        embeddings[index+1] = embedding_values\n",
        "\n",
        "        # Increase the number of words we have seen up to this point\n",
        "        index += 1\n",
        "\n",
        "# Open the dataset of tweets and their respective sentiments.\n",
        "# Once open, go through each tweet and convert it from a list of words into a list of integers. \n",
        "# Recall these integers represent the numerical position in \"embeddings_indexes\" where the word that was previously\n",
        "# in the tweet at that position is located.\n",
        "with open(data_file_name) as inputDataFile:\n",
        "\n",
        "    count = 0 # At this point we have not seen any tweets \n",
        "    max_tweet_length = 0\n",
        "\n",
        "    # Use the built in CSV reader to read from the file containing the tweet data\n",
        "    csvreader = csv.reader(inputDataFile)\n",
        "\n",
        "    # Skip over the header of the CSV\n",
        "    next(csvreader)\n",
        "\n",
        "    for rowOfData in csvreader:\n",
        "\n",
        "        count += 1 # Increase the number of tweets we have seen\n",
        "\n",
        "        # Create an empty list to store the list of integers for tokens that we find in tweets \n",
        "        singleVector= []\n",
        "    \n",
        "        # Retrieve useful information from each data row\n",
        "        # This includes the annotated sentiment and the actual text of the tweet\n",
        "        classifiedSentiment = rowOfData[0]\n",
        "        tweetText = rowOfData[2]\n",
        "\n",
        "        # If the tweet is empty or the sentiment is missing skip over the tweet\n",
        "        if len(tweetText) == 0 or len(classifiedSentiment) == 0:\n",
        "            continue\n",
        "\n",
        "        # Tokenize the tweet, that is split the tweet into its individual words or tokens.\n",
        "        tweetTokens = tweetText.split()\n",
        "        numberOfTokens = len(tweetTokens)\n",
        "\n",
        "        # For each token in a tweet:\n",
        "        for tok in tweetTokens:\n",
        "\n",
        "            # Check to see if this is the longest tweet we have seen. \n",
        "            # This is important because later the neural network needs to have a fixed maximum length \n",
        "            # and every tweet needs to be of this length! \n",
        "            if max_tweet_length < len(tweetTokens):\n",
        "                max_tweet_length = len(tweetTokens)\n",
        "\n",
        "            # Check if the token has a static word embedding, if it does save the position of that word embedding to a list\n",
        "            # This position is relative to the embeddings collection that was created earlier.\n",
        "            if tok in embeddings_indexes:\n",
        "                singleVector.append(embeddings_indexes[tok])\n",
        "            \n",
        "        # Account for the possibility that none of the tokens within the tweet had a static word embedding\n",
        "        if len(singleVector) == 0:\n",
        "            continue\n",
        "\n",
        "        # In the original dataset the positive and negative sentiment classes were 0 and 4. \n",
        "        # Thus for binary classification we convert labels to proper label values of 0 and 1\n",
        "        classifiedSentiment = 1 if int( classifiedSentiment ) > 0 else 0\n",
        "        \n",
        "        # Store the list of integers and the annotated sentiment to their respective lists\n",
        "        tweetVectors.append(singleVector)\n",
        "        classifications.append([classifiedSentiment])\n",
        "\n",
        "# Since the neural network expects all tweets to be of the same size and this is not naturally true, \n",
        "# padding the end of tweets with zeros was necessary. These zeros correspond to a \"padding\" static word \n",
        "# embedding within the \"embeddings\" colelction.\n",
        "tweetVectors = pad_sequences(tweetVectors, maxlen=max_tweet_length, padding=\"post\")\n",
        "\n",
        "# Conver the lists of integers and sentiments into numpy arrays\n",
        "tweetVectors = np.asarray(tweetVectors,dtype=np.int32)\n",
        "classifications = np.asarray(classifications,dtype=np.int32)\n",
        "\n",
        "# For debugging purposes we inlcude a visual check that the shape of the two arrays are correct\n",
        "print(tweetVectors.shape)\n",
        "print(classifications.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CESZR_EPBrLY"
      },
      "source": [
        "# Building The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck8XWjveBqWJ",
        "outputId": "5dfdf354-e8bb-4867-81ce-edc0d7b84931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "building model\n",
            "\n",
            "fitting model\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 49, 200)           238703000 \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 49, 32)            19232     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 24, 32)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 768)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               196864    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 238,952,121\n",
            "Trainable params: 238,952,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print('\\nbuilding model')\n",
        "\n",
        "# Define the Model\n",
        "# We use a sequential convolutional neural network\n",
        "model = Sequential()\n",
        "\n",
        "# We use an embedding layer so that the neural network creates the numerical matrix for each tweet.\n",
        "# As mentioned above, this is done by using \"embeddings\" and \"embeddings_indexes\" to conver the lists of \n",
        "# integers into a two dimensional numerical matrix that represents each tweet \n",
        "model.add(Embedding(numberOfEmbeddings, dimensions, weights=[embeddings],input_length=max_tweet_length, trainable=False))\n",
        "\n",
        "\n",
        "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# An optimizer is selected with the set learning rate, the loss function is selected, and the model is compiled. \n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# The model is fit and a summary of the structure is printed to the screen for visual inspection.\n",
        "print('\\nfitting model')\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUxuNHSGBvqh"
      },
      "source": [
        "# Train The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpzCD6oYzoc5",
        "outputId": "a4de951b-d161-4239-cd0d-63563c7cbafa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data Shapes\n",
            "(1270930, 49)\n",
            "(1270930, 1)\n",
            "Validation Data Shapes\n",
            "(158867, 49)\n",
            "(158867, 1)\n",
            "Evaluation Data Shapes\n",
            "(158866, 49)\n",
            "(158866, 1)\n",
            "Epoch 1/10\n",
            "1655/1655 [==============================] - 121s 72ms/step - loss: 0.5071 - accuracy: 0.7481 - val_loss: 0.4775 - val_accuracy: 0.7702\n",
            "Epoch 2/10\n",
            "1655/1655 [==============================] - 119s 72ms/step - loss: 0.4627 - accuracy: 0.7802 - val_loss: 0.4642 - val_accuracy: 0.7793\n",
            "Epoch 3/10\n",
            "1655/1655 [==============================] - 118s 72ms/step - loss: 0.4481 - accuracy: 0.7895 - val_loss: 0.4595 - val_accuracy: 0.7828\n",
            "Epoch 4/10\n",
            "1655/1655 [==============================] - 119s 72ms/step - loss: 0.4370 - accuracy: 0.7963 - val_loss: 0.4569 - val_accuracy: 0.7847\n",
            "Epoch 5/10\n",
            "1655/1655 [==============================] - 118s 72ms/step - loss: 0.4268 - accuracy: 0.8023 - val_loss: 0.4568 - val_accuracy: 0.7846\n",
            "Epoch 6/10\n",
            "1655/1655 [==============================] - 118s 72ms/step - loss: 0.4170 - accuracy: 0.8078 - val_loss: 0.4579 - val_accuracy: 0.7838\n",
            "Epoch 7/10\n",
            "1655/1655 [==============================] - 119s 72ms/step - loss: 0.4072 - accuracy: 0.8136 - val_loss: 0.4612 - val_accuracy: 0.7830\n",
            "Epoch 8/10\n",
            "1655/1655 [==============================] - 119s 72ms/step - loss: 0.3974 - accuracy: 0.8193 - val_loss: 0.4695 - val_accuracy: 0.7815\n",
            "Epoch 9/10\n",
            "1655/1655 [==============================] - 119s 72ms/step - loss: 0.3873 - accuracy: 0.8249 - val_loss: 0.4724 - val_accuracy: 0.7816\n",
            "Epoch 10/10\n",
            "1655/1655 [==============================] - 118s 71ms/step - loss: 0.3769 - accuracy: 0.8307 - val_loss: 0.4835 - val_accuracy: 0.7787\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f722b674390>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split the data set into three pieces for use in training and evaluation of the model\n",
        "# We use an 80% train, 10% development or validation, and 10% test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweetVectors, classifications, test_size=0.2, random_state=42)\n",
        "X_test, X_val, y_test, y_val  = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "# For visual inspection the shape of each data set is printed to the screen\n",
        "print('Training Data Shapes')\n",
        "print(X_train.shape) \n",
        "print(y_train.shape)\n",
        "\n",
        "print('Validation Data Shapes')\n",
        "print(X_val.shape) \n",
        "print(y_val.shape) \n",
        "\n",
        "print('Evaluation Data Shapes')\n",
        "print(X_test.shape) \n",
        "print(y_test.shape) \n",
        "\n",
        "# The model is trained using the training data, the selected number of epochs, and batch size. \n",
        "# The validation set is used to determine how well the model is performing on data that it has not \n",
        "# and will not see for training. \n",
        "model.fit(X_train, y_train, epochs=epochs, shuffle=True, verbose=1, batch_size=bs, validation_data=(X_val,y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1-1WTPUFhRa"
      },
      "source": [
        "# Perform Automated Evaluation on the Test Data Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nTyqP4mFgYh",
        "outputId": "70335a91-602c-41ea-c1a5-32e461bea49d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "evaluating model\n",
            "\n",
            "Test Accuracy: 77.890801\n"
          ]
        }
      ],
      "source": [
        "# Evaluation of perfomance of the model using the built in functions\n",
        "print('\\nevaluating model')\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('\\nTest Accuracy: %f' % (acc*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Perform Manual Evaluation on the Test Data Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FulTtXhl5W42",
        "outputId": "f8cdb0c9-efa5-4a8a-bf15-0942d0a59397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "71000\n",
            "72000\n",
            "73000\n",
            "74000\n",
            "75000\n",
            "First 75000\n",
            "True Negative: 28886\n",
            "True Positive: 29525\n",
            "False Negative: 7989\n",
            "False Positive: 8600\n"
          ]
        }
      ],
      "source": [
        "# This code is used to calculate the values of a confusion matrix. With the following four values\n",
        "true_negative = 0\n",
        "true_positive = 0\n",
        "false_negative = 0\n",
        "false_positive = 0\n",
        "# precision, recall, and f1 scores can be calculated\n",
        "\n",
        "X_test_tensor = tf.convert_to_tensor(X_test)\n",
        "predicted_values = model.call(X_test_tensor)\n",
        "\n",
        "count = 0\n",
        "\n",
        "# Go through all of the predictions from the model\n",
        "for prediction, y_label in zip( predicted_values, y_test ):\n",
        "\n",
        "  count += 1 # increase the number of predictions we have seen\n",
        "\n",
        "  # Print the progress in manual evaluation every 1000 samples.\n",
        "  if (count % 1000) == 0:\n",
        "    print(count)  \n",
        "  \n",
        "  # Since the predictions are between 0 and 1, \n",
        "  # If they are are closer to 1, greater than or equal to 0.5, we say it was predicted as a 1\n",
        "  # Otherwise we say it was predicted as a 0\n",
        "  prediction = 1 if prediction >= 0.5 else 0\n",
        "\n",
        "  # If the prediction and true label match, that is the prediction is correct \n",
        "  # we count it as a true class prediction for whichever label it was\n",
        "  if prediction == y_label[0]: \n",
        "    if y_label[0] == 0:\n",
        "      true_negative += 1\n",
        "    else:\n",
        "      true_positive += 1\n",
        "  # Otherwise if the prediction and true label do not match, that is the prediction is incorrect\n",
        "  # we count it as a false class prediction depending of the label it was supposed to be\n",
        "  else:\n",
        "    if prediction == 0:\n",
        "      false_negative += 1\n",
        "    else:\n",
        "      false_positive += 1\n",
        "\n",
        "# Print the counts of the confusion matrix classes\n",
        "# These can be used to calculate precision, recall, and f1 scores\n",
        "print(f'True Negative: {true_negative}')\n",
        "print(f'True Positive: {true_positive}')\n",
        "print(f'False Negative: {false_negative}')\n",
        "print(f'False Positive: {false_positive}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.14 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
